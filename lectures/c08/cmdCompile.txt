# Linux UBUNTU 16:

# //set env variables:

export JAVA_HOME=/opt/software/java/jdks/jdk-9.0.4
#export JAVA_HOME=/opt/software/java/jdks/jdk1.8.0_161
export CATALINA_HOME=/opt/software/apache-tomcat-9.0.31
#export CATALINA_HOME=/opt/software/apache-tomcat-8.0.35
export PATH=$JAVA_HOME/bin:$CATALINA_HOME/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games
export CLASSPATH=".:$CATALINA_HOME/bin/bootstrap.jar:$CATALINA_HOME/bin/tomcat-juli.jar:$CATALINA_HOME/lib/*"
export DAD=/home/stud/dad/lectures

cd $DAD/c08


A. REST Services Services | JAX-RS

# 1. Stand alone JAX-RS server:
cd $DAD/c08/rest/s08_jweb_jaxrs_server/src
javac -cp .:../WebContent/WEB-INF/lib/* -d ../build eu/ase/java/*/*.java
cd ../build
mv * ../WebContent/WEB-INF/classes
cd ../WebContent
jar -cvf ../build/srest.war *
cd ..
cp ./build/srest.war $CATALINA_HOME/webapps/srest.war
rm ./build/srest.war

$CATALINA_HOME/bin/startup.sh

# in Postman (Mozilla Firefox): 
# https://www.pegaxchange.com/2016/08/11/jax-rs-java-rest-service-eclipse-tomcat/
# http://127.0.0.1:8080/srest/restservices/productcatalog/search/category/electronics
# http://127.0.0.1:8080/srest/restservices/productcatalog/search/category/hardware
# http://127.0.0.1:8080/srest/restservices/productcatalog/search/category/books

# http://127.0.0.1:8080/srest/restservices/productcatalog/search?name=Hammer

# insert into Postman in Chrome using POST instead of GET:
# http://127.0.0.1:8080/srest/restservices/productcatalog/insert
/*
Content-Type: application/json

{
 "id":11,
 "name":"Drill",
 "category":"Hardware",
 "unitPrice":294.39
}
 */

$CATALINA_HOME/bin/shutdown.sh

rm -rf $CATALINA_HOME/webapps/srest
rm -rf $CATALINA_HOME/webapps/srest.war


B. HTTP2 vs 1 - Tomcat server & Java/Browser client
# The server may have CONFIGURATION ISSUES
# https://readlearncode.com/configure-tomcat-9-for-http2/
# https://gist.github.com/hasalex/7db64e64c77d1ce996c4b9ba89732f7f

# sudo apt-get install libtcnative-1
# sudo ln -sv /usr/lib/x86_64-linux-gnu/libtcnative-1.so /usr/lib/

# Security is not mandatory
# cd $CATALINA_HOME/conf/certificates
# openssl req -newkey RSA:2048 -nodes -keyout tomcat.key -x509 -days 3650 -out tomcat.crt
# openssl pkcs12 -inkey tomcat.key -in tomcat.crt -export -out tomcat.pfx
# entered pass stud

<Connector port="8444" protocol="org.apache.coyote.http11.Http11AprProtocol"
               maxThreads="150" SSLEnabled="true" >
        <UpgradeProtocol className="org.apache.coyote.http2.Http2Protocol" />
<!--
        <SSLHostConfig>
            <Certificate certificateKeyFile="conf/certificates/tomcat.key"
                         certificateFile="./conf/certificates/tomcat.crt"
                         type="RSA" />
        </SSLHostConfig>
-->
    </Connector>


$CATALINA_HOME/bin/startup.sh


cd $DAD/c08/http2client
javac --add-modules jdk.incubator.httpclient eu/ase/http2client/ProgMainHttp2Client.java
java --add-modules jdk.incubator.httpclient eu.ase.http2client.ProgMainHttp2Client

$CATALINA_HOME/bin/shutdown.sh

C. OpenMPI - Messages Passing Interface (HTC+HPC - Distributed & Parallel vs. Parallel Systems)

# *** OpenMP is used ONLY for shared memory parallelism, therefore not directly in Distributed Systems
#############################################################################

# OpenMP examples for Parallel Computing Intro:

cd $DAD/c08/src/HPC_OpenMP

g++ -fopenmp -O2 ProgMainOpenMpTest01.cpp -o ProgMainOpenMpTest01
./ProgMainOpenMpTest01


g++ -fopenmp -O2 ProgMainOpenMpTest02.cpp -o ProgMainOpenMpTest02
./ProgMainOpenMpTest02


g++ -fopenmp -O2 ProgMainOpenMpTest03.cpp -o ProgMainOpenMpTest03
./ProgMainOpenMpTest03

#############################################################################
# OpenMPI maybe run in CaaS - Docker, IaaS - Oracle VMBox, Public Cloud - AWS EC2 (Amazon)
#
docker search critoma
# https://hub.docker.com/r/critoma/linux-ubuntu-dev-security-ism
docker pull critoma/linux-ubuntu-dev-security-ism
docker run -it critoma/linux-ubuntu-dev-security-ism

$ docker container ls -a
$ docker start <container_id>
$ docker exec -it <container_id> /bin/bash
$ docker exec -u stud -t -i 0aa85c74361c /bin/bash

# deprecated:
# docker pull critoma/ubuntu-java-node-py-dev
# docker run -it critoma/ubuntu-java-node-py-dev

# alternatively
docker pull ubuntu


docker container ls -a

docker exec -it <container_id> /bin/bash
e.g.
docker exec -u stud -t -i 0aa85c74361c /bin/bash

# download and extract Open-MPI
# https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.2.tar.gz
# wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.2.tar.gz
# https://www.open-mpi.org/faq/?category=java

# tar -xf <openmpi_archive>
# sudo mv <openmpi_dir> /opt/software
# cd /opt/software/<openmpi_dir>
# export JAVA_HOME=<JDK_INSTALL_DIR e.g. /opt/software/java/jdk-17.0.2>
# ./configure --enable-mpi-java --with-jdk-bindir=$JAVA_HOME/bin --with-jdk-headers=$JAVA_HOME/include
# make
# sudo make install

# sudo find / -name mpicc
# sudo chown ubuntu /usr/local/bin/mpicc
# sudo cp -R /usr/local/share/openmpi /usr/share/
# sudo chown -R ubuntu /usr/local/share/openmpi
# sudo chown -R ubuntu /usr/share/openmpi
# sudo chown -R ubuntu /usr/local/bin/mpi*
# sudo chown -R ubuntu /usr/local/bin/*mpi*
# sudo cp -R /usr/local/bin/mpi* /usr/bin
# sudo cp -R /usr/local/bin/*mpi* /usr/bin
# sudo chown ubuntu /usr/local/bin/opal_wrapper
# sudo chown -R ubuntu /usr/local/bin/orte*
# ubuntu@ip-172-31-34-167:/opt/software/openmpi-4.1.2$ sudo cp /usr/local/bin/opal_wrapper /usr/bin
# ubuntu@ip-172-31-34-167:/opt/software/openmpi-4.1.2$ sudo cp /usr/local/bin/orte* /usr/bin
# ubuntu@ip-172-31-34-167:/opt/software/openmpi-4.1.2$ sudo chown -R ubuntu /usr/bin/orte*
# sudo chown -R ubuntu /usr/bin/opal_wrapper 
# mpicc --version && mpijavac -version

# Sample Java bindings MPI program:
package eudicejmpi;

import mpi.*;

public class TestMpi {
 public static void main(String[] args) {
  try {
   MPI.Init(args);
   int myrank = MPI.COMM_WORLD.getRank();
   int size = MPI.COMM_WORLD.getSize();
   System.out.println("myrank = " + myrank + ", size = " + size);
 
   if(myrank == 0) {
     char[] message = "Hello, there".toCharArray();
     MPI.COMM_WORLD.send(message, message.length, MPI.CHAR, 1, 99);
   }
   else {
     char[] message = new char [20] ;
     MPI.COMM_WORLD.recv(message, 20, MPI.CHAR, 0, 99);
     System.out.println("received:" + new String(message) + ":");
   }
   MPI.Finalize();
  } catch(MPIException mpie) {
     mpie.printStackTrace();
  }
 }
}

# /opt/software/java/jdk-17.0.2/bin/javac -classpath .:/usr/local/lib:/opt/software/java/jdk-17.0.2/lib:/usr/local/lib/mpi.jar:/usr/local/lib/shmem.jar eudicejmpi/TestMpi.java
# nodefile txt file in the current folder should contain:
node1 slots=1
node2 slots=1

# put the RSA private key for ssh in ~/.ssh/id_rsa
# have all the nodes with Java bindings for OpenMPI
# mpirun /opt/software/java/jdk-17.0.2/bin/java -classpath .:/usr/local/lib:/opt/software/java/jdk-17.0.2/lib:/usr/local/lib/mpi.jar:/usr/local/lib/shmem.jar eudicejmpi.TestMpi
# mpirun -np 2 -hostfile ./nodefile /opt/software/java/jdk-17.0.2/bin/java -classpath .:/usr/local/lib:/opt/software/java/jdk-17.0.2/lib:/usr/local/lib/mpi.jar:/usr/local/lib/shmem.jar eudicejmpi.TestMpi
# MANDATORY to do ssh on each node including the node itself - e.g. ssh stud@node1 and ssh stud@node2 (it is maybe a ssh bug or known host keys) - this part may have automation with DevOps


# or alternatively in Linux Ubuntu 16/18/20:
sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev

#Ubuntu 20: https://ubuntu.pkgs.org/20.04/ubuntu-universe-arm64/openmpi-bin_4.0.3-0ubuntu1_arm64.deb.html
Update the package index:
# sudo apt-get update
Install openmpi-bin deb package:
# sudo apt-get install openmpi-bin
#sudo apt-get install build-essential openmpi-bin openmpi-common openssh-client openssh-server libopenmpi-dev


# configure ssh in authentication without password - Docker or VM is the same:
nano /etc/hosts # insert proper IPs for node1 and node2
# https://mhelp.pro/how-to-configure-ssh-certificates-for-login-to-ubuntu/
nano /etc/ssh/sshd_config
# change 3 lines: Authorized keys, Password No AND certificates yes

ssh-keygen -t rsa
ssh-keygen -f ~/.ssh/id_rsa -p
ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0700 .ssh
chmod 400 .ssh/id_rsa
cp .ssh/id_rsa.pub .ssh/authorized_keys
cp .ssh/id_rsa.pub .ssh/authorized_keys2

service ssh restart

ssh root@node2
or
ssh ubuntu@node2

############################################################################
# in Amazon Linux (RHEL) in EC2 Cluster:

# in Browser (e.g. Google Chrome)
# https://us-east-2.console.aws.amazon.com/console/home?region=us-east-2#
# secitc@gmail.com

https://console.aws.amazon.com/ecs/
https://us-east-2.console.aws.amazon.com/ecs/home?region=us-east-2#/clusters
https://us-east-2.console.aws.amazon.com/ecs/home?region=us-east-2#/clusters/create/new
# create key pair
https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#KeyPairs:sort=keyName
https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#KeyPairs:


# in terminal MacOS/Linux - cd ~/Downloads
chmod 400 clusterAwsEc2_01.pem
ssh -i "clusterAwsEc2_01.pem" root@ec2-3-135-9-20.us-east-2.compute.amazonaws.com
ssh -i "clusterAwsEc2_01.pem" ec2-user@ec2-3-135-9-20.us-east-2.compute.amazonaws.com

ssh -i "clusterAwsEc2_01.pem" ec2-user@ec2-18-216-192-115.us-east-2.compute.amazonaws.com


https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html#efa-start-impi
https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#Instances:sort=instanceId


ssh -i "clusterAwsEc2_01.pem" ec2-user@ec2-18-191-38-27.us-east-2.compute.amazonaws.com
ssh -i "clusterAwsEc2_01.pem" ec2-user@ec2-3-14-14-180.us-east-2.compute.amazonaws.com

# https://glennklockwood.blogspot.com/2013/04/quick-mpi-cluster-setup-on-amazon-ec2.html

# sudo yum provides */mpicc
# sudo yum install openmpi
# ls -latr /usr/lib64/openmpi
# sudo yum install openmpi-devel
# create the C code with source testmpi.c:

#include <mpi.h>
#include <stdio.h>
/*
#include <stdio.h>
#include <mpi.h>

main(int argc, char **argv)
{
   int node;
   MPI_Init(&argc,&argv);
   MPI_Comm_rank(MPI_COMM_WORLD, &node);
   printf("Hello World from Node %d\n",node);
   MPI_Finalize();
}
*/


int main(int argc, char** argv) {
    // Initialize the MPI environment
    MPI_Init(NULL, NULL);

    // Get the number of processes
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Get the rank of the process
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // Get the name of the processor
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    // Print off a hello world message
    printf("Hello world from processor %s, rank %d out of %d processors\n",
           processor_name, world_rank, world_size);

    // Finalize the MPI environment.
    MPI_Finalize();
}


# MANDATORY to do ssh on each node including the node itself - e.g. ssh stud@node1 and ssh stud@node2 (it is maybe a ssh bug or known host keys) - this part may have automation with DevOps
# /usr/lib64/openmpi/bin/mpicc testopenmpi.c -o testopenmpi
# /usr/lib64/openmpi/bin/mpirun ./testopenmpi

# MANDATORY to do ssh on each node including the node itself - e.g. ssh stud@node1 and ssh stud@node2 (it is maybe a ssh bug or known host keys) - this part may have automation with DevOps
mpirun -np 2 -hostfile ~/nodefile ./testopenmpi



######################################
Cristians-MBP-2:Downloads ctoma$ ssh -i "clusterAwsEc2_01.pem" ec2-user@ec2-18-191-38-27.us-east-2.compute.amazonaws.com
Last login: Wed Mar  4 09:12:41 2020 from 213.233.110.155

       __|  __|_  )
       _|  (     /   Amazon Linux 2 AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-2/
3 package(s) needed for security, out of 34 available
Run "sudo yum update" to apply all updates.
-bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory


Last login: Sun Mar  8 15:41:40 on ttys025
Cristians-MBP-2:Downloads ctoma$ ssh -i "clusterAwsEc2_01.pem" ec2-user@ec2-3-14-14-180.us-east-2.compute.amazonaws.com
Last login: Wed Mar  4 09:14:34 2020 from node1

       __|  __|_  )
       _|  (     /   Amazon Linux 2 AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-2/
3 package(s) needed for security, out of 34 available
Run "sudo yum update" to apply all updates.
-bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory
[ec2-user@ip-172-31-33-118 ~]$ ll
total 16
-rw-rw-r-- 1 ec2-user ec2-user   29 Mar  4 09:13 nodefile
-rwxrwxr-x 1 ec2-user ec2-user 8456 Mar  4 09:13 testopenmpi
[ec2-user@ip-172-31-33-118 ~]$ cat nodefile 
node1 slots=1
node2 slots=1

[ec2-user@ip-172-31-33-118 ~]$ cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost6 localhost6.localdomain6
172.31.45.35 node1
172.31.33.118 node2

[ec2-user@ip-172-31-33-118 ~]$ uname -a
Linux ip-172-31-33-118.us-east-2.compute.internal 4.14.165-131.185.amzn2.x86_64 #1 SMP Wed Jan 15 14:19:56 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
[ec2-user@ip-172-31-33-118 ~]$ 

###
mpicc scatter.c -o scatter
scp scatter ec2-user@node2:/home/ec2-user
mpirun -np 2 -hostfile ~/nodefile ./scatter

############################################################################

cd $DAD/c08/openmpi
mpicc testmpi.c -o testmpi
mpirun ./testmpi
# vs.
./testmpi

mpicc testsend_recv.c -o testsend_recv
mpirun ./testsend_recv
mpirun -np 2 ./testsend_recv

mpicc scatter.c -o scatter
#mpirun ./scatter
mpirun -np 2 ./scatter


#################################################################################
# with 3 instances of Linux Ubuntu 18.04 LTS in Amazon AWS EC2 cluster:



#####
# https://console.aws.amazon.com/ec2/
# https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#Instances:sort=instanceId
# Private IPs: 172.31.45.37, 172.31.45.38 ,    172.31.45.39
# get the connectivity strings after each reboot from Amazon EC2 dashboard:

ssh -i "clusterAwsEc2_01.pem" ubuntu@ec2-18-191-196-121.us-east-2.compute.amazonaws.com
ssh -i "clusterAwsEc2_01.pem" ubuntu@ec2-18-191-60-84.us-east-2.compute.amazonaws.com
ssh -i "clusterAwsEc2_01.pem" ubuntu@ec2-18-224-37-212.us-east-2.compute.amazonaws.com

sudo apt install libopenmpi-dev

lscpu
uname -a
free --mega
ifconfig

# on first 2 nodes:
sudo nano /etc/hosts
172.31.45.37 node1
172.31.45.38 node2

# in first node, only once: 
cd /home/ubuntu/openmpitest
nano nodefile
node1 slots=1
node2 slots=1


mpicc -o testopenmpi.elf64 testopenmpi.c
mpicc -o scatter.elf64 scatter.c

cd /home/ubuntu
touch .ssh/clusterAwsEc2_01.pem
chmod 600 .ssh/clusterAwsEc2_01.pem
nano .ssh/clusterAwsEc2_01.pem
...
cp .ssh/clusterAwsEc2_01.pem ~/.ssh/id_rsa


scp -i "./.ssh/clusterAwsEc2_01.pem" -r openmpitest ubuntu@node2:/home/ubuntu
ssh -i ".ssh/clusterAwsEc2_01.pem" ubuntu@node2
ssh ubuntu@node2
ssh ubuntu@node1

mpirun -np 1 ./testopenmpi.elf64
mpirun -np 2 -hostfile ./nodefile ./testopenmpi.elf64
mpirun -np 2 -hostfile ./nodefile ./scatter.elf64
mpirun -mca btl_base_warn_component_unused 0  -np 2 -hostfile ./nodefile ./scatter.elf64

# Infiniband Mellanox drivers at byte level from 
# https://www.mellanox.com/pdf/prod_software/Ubuntu_18_04_Inbox_Driver_User_Manual.pdf 
# have issues in Ubuntu and the command line can avoid this:
mpirun --mca btl ^openib -np 2 -hostfile ./nodefile ./scatter.elf64


### debug in Amazon EC2 instances:
# Security Groups - see inbound and outbound rules:
https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#SecurityGroups:
# Save snapshots - be aware about invoice for free tire account - only 30 GB overall storage is allowed (including the standard storage):
https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#Snapshots:visibility=owned-by-me;sort=snapshotId
# Manage volumes - attach to one or another instance macjine:
https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#Volumes:sort=desc:createTime
# Console dashboad: - pay attention to stop all instances otherwise a 200 usd+ bill to pay and unfortunately after each restart
# the CONNECTIVITY STRING VIA SSH AND PUBLIC IP are CHANGED!
https://console.aws.amazon.com/ec2/

ubuntu@ip-172-31-45-38:~$ sudo mkdir /media/newhd
sudo mount /dev/xvdf1 /media/newhd
ls /media/newhd/home/ubuntu
sudo umount /dev/xvdf1
 


